{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense, Bidirectional\n",
    "from keras.layers import Activation, dot, concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm_notebook\n",
    "import nltk\n",
    "%matplotlib inline  \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "seaborn.set(font=['AppleMyungjo'], font_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154304, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.read_csv('dataset/english-german-train.csv')\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.english = lines.english.apply(lambda x: x.lower())\n",
    "lines.german = lines.german.apply(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "lines.english = lines.english.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.german = lines.german.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.english = lines.english.apply(lambda x: x.strip())\n",
    "lines.german = lines.german.apply(lambda x: x.strip())\n",
    "\n",
    "lines.english = lines.english.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines.german = lines.german.apply(lambda x: re.sub(\" +\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.english = lines.english.apply(lambda x: re.sub(\"\\?\\?\", '', x))\n",
    "lines.german = lines.german.apply(lambda x: re.sub(\"\\?\\?\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.german = lines.german.apply(lambda x : 'START_ '+ x + '_END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['english', 'german'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lines['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74580</th>\n",
       "      <td>you know where to find me if you want to talk</td>\n",
       "      <td>START_ du weit ja wo ich zu finden bin wenn du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14873</th>\n",
       "      <td>tom is no longer studying french</td>\n",
       "      <td>START_ tom lernt nicht mehr franzosisch_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15322</th>\n",
       "      <td>im awfully afraid of heights</td>\n",
       "      <td>START_ ich habe groe hohenangst_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69292</th>\n",
       "      <td>i thought tom loved mary</td>\n",
       "      <td>START_ ich dachte tom liebe maria_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17142</th>\n",
       "      <td>he is unquestionably the oldest man in the vil...</td>\n",
       "      <td>START_ er ist ohne zweifel der alteste mann im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82625</th>\n",
       "      <td>there is a fly on the ceiling</td>\n",
       "      <td>START_ da ist eine fliege auf der zimmerdecke_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92468</th>\n",
       "      <td>i know tom has been hurt</td>\n",
       "      <td>START_ ich wei dass tom verletzt wurde_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70032</th>\n",
       "      <td>ill give that to tom</td>\n",
       "      <td>START_ ich werde das tom geben_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19522</th>\n",
       "      <td>my grandmother had a stroke</td>\n",
       "      <td>START_ meine gromutter hatte einen schlaganfal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49097</th>\n",
       "      <td>youve got the wrong number</td>\n",
       "      <td>START_ sie haben sich verwahlt_END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 english  \\\n",
       "74580      you know where to find me if you want to talk   \n",
       "14873                   tom is no longer studying french   \n",
       "15322                       im awfully afraid of heights   \n",
       "69292                           i thought tom loved mary   \n",
       "17142  he is unquestionably the oldest man in the vil...   \n",
       "82625                      there is a fly on the ceiling   \n",
       "92468                           i know tom has been hurt   \n",
       "70032                               ill give that to tom   \n",
       "19522                        my grandmother had a stroke   \n",
       "49097                         youve got the wrong number   \n",
       "\n",
       "                                                  german  \n",
       "74580  START_ du weit ja wo ich zu finden bin wenn du...  \n",
       "14873        START_ tom lernt nicht mehr franzosisch_END  \n",
       "15322                START_ ich habe groe hohenangst_END  \n",
       "69292              START_ ich dachte tom liebe maria_END  \n",
       "17142  START_ er ist ohne zweifel der alteste mann im...  \n",
       "82625  START_ da ist eine fliege auf der zimmerdecke_END  \n",
       "92468         START_ ich wei dass tom verletzt wurde_END  \n",
       "70032                 START_ ich werde das tom geben_END  \n",
       "19522  START_ meine gromutter hatte einen schlaganfal...  \n",
       "49097                 START_ sie haben sich verwahlt_END  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_english = set()\n",
    "for sent in lines.english:\n",
    "    for word in sent.split():\n",
    "        if word not in vocab_english:\n",
    "            vocab_english.add(word)\n",
    "\n",
    "vocab_german = set()\n",
    "for sent in lines.german:\n",
    "    for word in sent.split():\n",
    "        if word not in vocab_german:\n",
    "            vocab_german.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 7.4321534114475325)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_list=[]\n",
    "for l in lines.german:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_length_german = np.max(length_list)\n",
    "max_length_german, np.average(length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 6.399587826627955)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_list=[]\n",
    "for l in lines.english:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_length_english = np.max(length_list)\n",
    "max_length_english, np.average(length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14693, 39096)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(vocab_english))\n",
    "target_words = sorted(list(vocab_german))\n",
    "num_encoder_tokens = len(vocab_english)\n",
    "num_decoder_tokens = len(vocab_german)\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39097"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85539</th>\n",
       "      <td>were you crying</td>\n",
       "      <td>START_ hast du geweint_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48004</th>\n",
       "      <td>tom looks just like john</td>\n",
       "      <td>START_ tom sieht genau aus wie john_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52460</th>\n",
       "      <td>tom does not know the difference between a dia...</td>\n",
       "      <td>START_ tom kennt nicht den unterschied zwische...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148762</th>\n",
       "      <td>are you in a bad mood</td>\n",
       "      <td>START_ sind sie schlechter laune_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133611</th>\n",
       "      <td>have you ever lost</td>\n",
       "      <td>START_ haben sie schon einmal verloren_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55401</th>\n",
       "      <td>you should learn how to ride a bicycle</td>\n",
       "      <td>START_ du solltest fahrrad fahren lernen_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57232</th>\n",
       "      <td>tom will cook</td>\n",
       "      <td>START_ tom wird kochen_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126572</th>\n",
       "      <td>tom didnt know what to do with it</td>\n",
       "      <td>START_ tom wusste nicht was er damit anfangen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103428</th>\n",
       "      <td>i always thought that a stroke was one of natu...</td>\n",
       "      <td>START_ ich dachte immer dass ein schlaganfall ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88035</th>\n",
       "      <td>does anyone have a lighter</td>\n",
       "      <td>START_ hat jemand ein feuerzeug_END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  english  \\\n",
       "85539                                     were you crying   \n",
       "48004                            tom looks just like john   \n",
       "52460   tom does not know the difference between a dia...   \n",
       "148762                              are you in a bad mood   \n",
       "133611                                 have you ever lost   \n",
       "55401              you should learn how to ride a bicycle   \n",
       "57232                                       tom will cook   \n",
       "126572                  tom didnt know what to do with it   \n",
       "103428  i always thought that a stroke was one of natu...   \n",
       "88035                          does anyone have a lighter   \n",
       "\n",
       "                                                   german  \n",
       "85539                          START_ hast du geweint_END  \n",
       "48004             START_ tom sieht genau aus wie john_END  \n",
       "52460   START_ tom kennt nicht den unterschied zwische...  \n",
       "148762               START_ sind sie schlechter laune_END  \n",
       "133611         START_ haben sie schon einmal verloren_END  \n",
       "55401        START_ du solltest fahrrad fahren lernen_END  \n",
       "57232                          START_ tom wird kochen_END  \n",
       "126572  START_ tom wusste nicht was er damit anfangen ...  \n",
       "103428  START_ ich dachte immer dass ein schlaganfall ...  \n",
       "88035                 START_ hat jemand ein feuerzeug_END  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = shuffle(lines)\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154304, 154304)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english = list(lines.english)\n",
    "german = list(lines.german)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(english)):\n",
    "    x.append(str(english[i]))\n",
    "    y.append(str(german[i]))\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((138873,), (15431,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_term2),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_term2, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_embedding = Word2Vec.load('embeddings/skipgram-english-256.model')\n",
    "german_embedding = Word2Vec.load('embeddings/skipgram-german-256.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tok = Tokenizer()\n",
    "ger_tok = Tokenizer()\n",
    "\n",
    "eng_tok.fit_on_texts(english)\n",
    "ger_tok.fit_on_texts(german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 14693 is out of bounds for axis 0 with size 14693\n"
     ]
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "encoder_embedding_matrix = np.zeros((num_encoder_tokens, latent_dim))\n",
    "for word, i in eng_tok.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = english_embedding[word]\n",
    "        if embedding_vector is not None:\n",
    "            encoder_embedding_matrix[i] = embedding_vector\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# create a weight matrix for words in training docs\n",
    "decoder_embedding_matrix = np.zeros((num_decoder_tokens, latent_dim))\n",
    "for word, i in ger_tok.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = german_embedding[word]\n",
    "        if embedding_vector is not None:\n",
    "            decoder_embedding_matrix[i] = embedding_vector\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_outputs:  Tensor(\"lstm_1/transpose_2:0\", shape=(?, ?, 256), dtype=float32)\n",
      "state_h:  Tensor(\"lstm_1/while/Exit_2:0\", shape=(?, 256), dtype=float32)\n",
      "state_c:  Tensor(\"lstm_1/while/Exit_3:0\", shape=(?, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True, weights=[encoder_embedding_matrix])(encoder_inputs)\n",
    "\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "print('encoder_outputs: ', encoder_outputs)\n",
    "print('state_h: ', state_h)\n",
    "print('state_c: ', state_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True, weights=[decoder_embedding_matrix])\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "attention = dot([decoder_outputs, encoder_outputs], axes=[2, 2])\n",
    "\n",
    "attention = Activation('softmax', name='attention')(attention)\n",
    "\n",
    "context = dot([attention, encoder_outputs], axes=[2, 1], name='context')\n",
    "\n",
    "decoder_combined_context = concatenate([context, decoder_outputs], name='decoder_combined_context')\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    3761408     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    10008832    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  525312      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, None)   0           lstm_2[0][0]                     \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, None, None)   0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "context (Dot)                   (None, None, 256)    0           attention[0][0]                  \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_combined_context (Conca (None, None, 512)    0           context[0][0]                    \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 39097)  20056761    decoder_combined_context[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 34,877,625\n",
      "Trainable params: 34,877,625\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'eng_ger_nmt_weights'\n",
    "logging = TrainValTensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(os.path.join(log_dir, 'ep{epoch:03d}-val_loss{val_loss:.3f}-val_acc{val_acc:.3f}.h5'),\n",
    "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples//batch_size,\n",
    "                    callbacks=[logging, checkpoint, reduce_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('lstm_attn_nmt_model/ep060-val_loss3.489-val_acc0.610.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs] + encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "\n",
    "encoder_inf_input = Input(shape=(None, latent_dim))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "\n",
    "attention = dot([decoder_outputs2, encoder_inf_input], axes=[2, 2])\n",
    "print('Attention: ', attention)\n",
    "\n",
    "attention = Activation('softmax', name='attention')(attention)\n",
    "print('Softmax: ', attention)\n",
    "\n",
    "context = dot([attention, encoder_inf_input], axes=[2, 1])\n",
    "print('Context: ', context)\n",
    "\n",
    "decoder_combined_context = concatenate([context, decoder_outputs2])\n",
    "print('Combined Context: ', decoder_combined_context)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_combined_context) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs, encoder_inf_input] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.summary()\n",
    "encoder_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.summary()\n",
    "decoder_model.input, decoder_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    [encoder_output, h, c] = encoder_model.predict(input_seq)\n",
    "    states_value = [h, c]\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    \n",
    "    #return\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq, encoder_output] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 100):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = decoder_model.layers\n",
    "for l in layers:\n",
    "    print('%s\\tname:%s' % (str(l), l.name))\n",
    "    \n",
    "assert(model.layers[7] == model.get_layer('attention'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = decoder_model.get_layer('attention') # or model.layers[7]\n",
    "attention_model = Model(inputs=decoder_model.inputs, outputs=decoder_model.outputs + [attention_layer.output])\n",
    "\n",
    "print(attention_model)\n",
    "print(attention_model.output_shape, attention_model.input_shape)\n",
    "attention_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attent_and_generate(input_seq):\n",
    "    decoded_sentence = []\n",
    "    \n",
    "    [encoder_output, h, c] = encoder_model.predict(input_seq)\n",
    "    states_value = [h, c]\n",
    "    \n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "    \n",
    "    stop_condition = False\n",
    "    attention_density = []\n",
    "    index = []\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c, attention = attention_model.predict([target_seq, encoder_output] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence.append(sampled_char)\n",
    "    \n",
    "        if (sampled_char == '_END') or len(decoded_sentence) > 50:\n",
    "            stop_condition = True\n",
    "            \n",
    "        states_value = [h, c]\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        attention_density.append((sampled_char, attention[0][0]))\n",
    "      \n",
    "    return np.array(attention_density), ' '.join(decoded_sentence)\n",
    "\n",
    "\n",
    "def visualize(text, encoder_input):\n",
    "    attention_weights, decoded_sent = attent_and_generate(encoder_input)\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    mats = []\n",
    "    dec_inputs = []\n",
    "    for dec_ind, attn in attention_weights:\n",
    "        mats.append(attn[:len(text[0].split(' '))].reshape(-1))\n",
    "        dec_inputs.append(dec_ind)\n",
    "        \n",
    "    attention_mat = np.transpose(np.array(mats))\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(attention_mat)\n",
    "    ax.set_xticks(np.arange(attention_mat.shape[1]))\n",
    "    ax.set_yticks(np.arange(attention_mat.shape[0]))\n",
    "\n",
    "    ax.set_xticklabels([inp for inp in dec_inputs])\n",
    "    ax.set_yticklabels([w for w in str(text[0]).split(' ')])\n",
    "\n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "    plt.show()\n",
    "    return decoded_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(encoder_input, actual_output), _ = next(train_gen)\n",
    "print(X_train[k:k+1])\n",
    "decoded_sent = visualize(X_train[k:k+1], encoder_input)\n",
    "print('Input English Sentence:', X_train[k:k+1][0])\n",
    "print('Actual German:', y_train[k:k+1][0][6:-4])\n",
    "print('Predicted German:', decoded_sent[:-4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
